{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\skaushik047\\Desktop\\WORK\\DRL\\V2\\Excel_LLM\\drl_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "st.set_page_config(layout=\"wide\")\n",
    "import os\n",
    "from streamlit_extras.add_vertical_space import add_vertical_space\n",
    "from dataclasses import dataclass\n",
    "from tools.new_excel_agent import excel_llm_agent\n",
    "# Set the title for the Streamlit app\n",
    "# Create a file uploader in the sidebar\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "import pandas as pd\n",
    "import openai\n",
    "from tools.viz_plotly import get_primer,format_question,run_request\n",
    "import warnings\n",
    "from st_aggrid import AgGrid\n",
    "from st_aggrid.grid_options_builder import GridOptionsBuilder\n",
    "from streamlit_pandas_profiling import st_profile_report\n",
    "from ydata_profiling import ProfileReport\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_experimental.agents.agent_toolkits import create_pandas_dataframe_agent\n",
    "\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from pydantic.config import ConfigDict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:7: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\skaushik047\\AppData\\Local\\Temp\\2\\ipykernel_4484\\3430145268.py:6: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  folder_path = '.\\data\\client\\LLM'\n",
      "C:\\Users\\skaushik047\\AppData\\Local\\Temp\\2\\ipykernel_4484\\3430145268.py:7: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df1=pd.read_csv('.\\data\\client\\LLM Data_2.csv')\n",
      "C:\\Users\\skaushik047\\AppData\\Local\\Temp\\2\\ipykernel_4484\\3430145268.py:8: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df2=pd.read_csv('.\\data\\client\\LLM Data.csv')\n",
      "C:\\Users\\skaushik047\\AppData\\Local\\Temp\\2\\ipykernel_4484\\3430145268.py:9: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df3=pd.read_csv('.\\data\\client\\product master_old.csv')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Define the path to the directory containing the CSV files\n",
    "folder_path = '.\\data\\client\\LLM'\n",
    "df1=pd.read_csv('.\\data\\client\\LLM Data_2.csv')\n",
    "df2=pd.read_csv('.\\data\\client\\LLM Data.csv')\n",
    "df3=pd.read_csv('.\\data\\client\\product master_old.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Use glob to get all CSV file paths in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "# Initialize an empty list to store DataFrames\n",
    "dataframes = []\n",
    "\n",
    "# Loop through the list of CSV files and read each one into a DataFrame\n",
    "for file in csv_files:\n",
    "  df = pd.read_csv(file)\n",
    "  dataframes.append(df)\n",
    "\n",
    "dataframes.append(df1)\n",
    "dataframes.append(df2)\n",
    "dataframes.append(df3)\n",
    "\n",
    "file_list=[]\n",
    "file_list.append(\"LLM Data_2\")\n",
    "\n",
    "file_list.append(\"LLM Data\")\n",
    "\n",
    "file_list.append(\"product master\")\n",
    "\n",
    "selected_files=file_list.copy()\n",
    "\n",
    "selected_dataframes = [dataframes[file_list.index(file)] for file in selected_files]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\skaushik047\\Desktop\\WORK\\DRL\\V2\\Excel_LLM\\drl_venv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:212: UserWarning: WARNING! deployment_name is not default parameter.\n",
      "                    deployment_name was transferred to model_kwargs.\n",
      "                    Please confirm that deployment_name is what you intended.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "aoai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\") \n",
    "aoai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    deployment_name=\"gpt-4o\",\n",
    "    api_key=aoai_api_key,\n",
    "    azure_endpoint=aoai_endpoint,\n",
    "    api_version=aoai_api_version,\n",
    ")\n",
    "\n",
    "\n",
    "# You need to deploy your own embedding model as well as your own chat completion model\n",
    "embed_model = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    deployment_name=\"text-embedding-ada-002\",\n",
    "    api_key=aoai_api_key,\n",
    "    azure_endpoint=aoai_endpoint,\n",
    "    api_version=aoai_api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 8, 'total_tokens': 17}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_abc28019ad', 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {}}, id='run-2ddb9cd7-f0cb-4b96-bb03-cd6b8b31a236-0', usage_metadata={'input_tokens': 8, 'output_tokens': 9, 'total_tokens': 17})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "# from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "# tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "# This executes code locally, which can be unsafe\n",
    "python_repl_tool = PythonREPLTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "# from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "# tavily_tool = TavilySearchResults(max_results=5)\n",
    "\n",
    "# This executes code locally, which can be unsafe\n",
    "python_repl_tool = PythonREPLTool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str):\n",
    "    # Each worker node will be given a name and some tools.\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                system_prompt,\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools)\n",
    "    return executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=name)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "members = [\"Researcher\", \"Coder\"]\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\"\n",
    ")\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "options = [\"FINISH\"] + members\n",
    "# Using openai function calling can make output parsing easier for us\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role.\",\n",
    "    \"parameters\": {\n",
    "        \"title\": \"routeSchema\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"next\": {\n",
    "                \"title\": \"Next\",\n",
    "                \"anyOf\": [\n",
    "                    {\"enum\": options},\n",
    "                ],\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "\n",
    "supervisor_chain = (\n",
    "    prompt\n",
    "    | llm.bind_functions(functions=[function_def], function_call=\"route\")\n",
    "    | JsonOutputFunctionsParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "import operator\n",
    "from typing import Sequence, TypedDict\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION. PROCEED WITH CAUTION\n",
    "code_agent = create_agent(\n",
    "    llm,\n",
    "    [python_repl_tool],\n",
    "    \"You may generate safe python code to analyze data and generate charts using matplotlib.\",\n",
    ")\n",
    "code_node = functools.partial(agent_node, agent=code_agent, name=\"Coder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found edge starting at unknown node 'Coder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Finally, add entrypoint\u001b[39;00m\n\u001b[0;32m     32\u001b[0m workflow\u001b[38;5;241m.\u001b[39mset_entry_point(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupervisor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 34\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mworkflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m     37\u001b[0m     {\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     41\u001b[0m     }\n\u001b[0;32m     42\u001b[0m ):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__end__\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m s:\n",
      "File \u001b[1;32mc:\\Users\\skaushik047\\Desktop\\WORK\\DRL\\V2\\Excel_LLM\\drl_venv\\Lib\\site-packages\\langgraph\\graph\\state.py:402\u001b[0m, in \u001b[0;36mStateGraph.compile\u001b[1;34m(self, checkpointer, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[0;32m    399\u001b[0m interrupt_after \u001b[38;5;241m=\u001b[39m interrupt_after \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m    401\u001b[0m \u001b[38;5;66;03m# validate the graph\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;66;03m# prepare output channels\u001b[39;00m\n\u001b[0;32m    411\u001b[0m output_channels \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__root__\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschemas[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    419\u001b[0m     ]\n\u001b[0;32m    420\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\skaushik047\\Desktop\\WORK\\DRL\\V2\\Excel_LLM\\drl_venv\\Lib\\site-packages\\langgraph\\graph\\graph.py:335\u001b[0m, in \u001b[0;36mGraph.validate\u001b[1;34m(self, interrupt)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m source \u001b[38;5;129;01min\u001b[39;00m all_sources:\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;129;01mand\u001b[39;00m source \u001b[38;5;241m!=\u001b[39m START:\n\u001b[1;32m--> 335\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound edge starting at unknown node \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    337\u001b[0m \u001b[38;5;66;03m# assemble targets\u001b[39;00m\n\u001b[0;32m    338\u001b[0m all_targets \u001b[38;5;241m=\u001b[39m {end \u001b[38;5;28;01mfor\u001b[39;00m _, end \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_all_edges}\n",
      "\u001b[1;31mValueError\u001b[0m: Found edge starting at unknown node 'Coder'"
     ]
    }
   ],
   "source": [
    "# The agent state is the input to each node in the graph\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always\n",
    "    # be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str\n",
    "\n",
    "pandas_agent = create_pandas_dataframe_agent(\n",
    "    llm,\n",
    "    df1,\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "    # agent_type=\"openai-tools\",\n",
    "    allow_dangerous_code=True\n",
    ")\n",
    "pandas_node = functools.partial(agent_node, agent=pandas_agent, name=\"Analyst\")\n",
    "\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"Analyst\", pandas_node)\n",
    "\n",
    "for member in members:\n",
    "    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "# The supervisor populates the \"next\" field in the graph state\n",
    "# which routes to a node or finishes\n",
    "conditional_map = {k: k for k in members}\n",
    "conditional_map[\"FINISH\"] = END\n",
    "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "# Finally, add entrypoint\n",
    "workflow.set_entry_point(\"supervisor\")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "for s in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"summarize the columns in the dataframe provided to you\")\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
